# DeepFake-Detection-Using-Vision-and-Wav2Vec2-Transformer-Models

The proliferation of deepfakes, hyper-realistic synthetic videos generated by AI, poses a significant threat to trust in online media and information. Accurate and robust detection of deepfakes is crucial to mitigate their potential for harm. This project explores a novel approach to deepfake detection by combining the power of vision and audio analysis through a multimodal architecture leveraging both Vision Transformer and Wav2Vec2 Transformer models. The proposed model utilizes a Vision Transformer to extract visual features from video frames, effectively capturing subtle inconsistencies introduced by deepfake generation techniques. Meanwhile, a Wav2Vec2 Transformer analyzes the accompanying audio, identifying potential discrepancies in speech patterns and voice characteristics. The extracted features from both modalities are then fused and fed into a final classification layer, enabling the model to make a final judgment on the authenticity of the video.
