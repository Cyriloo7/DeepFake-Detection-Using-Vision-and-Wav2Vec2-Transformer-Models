# DeepFake-Detection-Using-Vision-and-Wav2Vec2-Transformer-Models

The proliferation of deepfakes, hyper-realistic synthetic videos generated by AI, poses a significant threat to trust in online media and information. Accurate and robust detection of deepfakes is crucial to mitigate their potential for harm. This project explores a novel approach to deepfake detection by combining the power of vision and audio analysis through a multimodal architecture leveraging both Vision Transformer and Wav2Vec2 Transformer models. The proposed model utilizes a Vision Transformer to extract visual features from video frames, effectively capturing subtle inconsistencies introduced by deepfake generation techniques. Meanwhile, a Wav2Vec2 Transformer analyzes the accompanying audio, identifying potential discrepancies in speech patterns and voice characteristics. The extracted features from both modalities are then fused and fed into a final classification layer, enabling the model to make a final judgment on the authenticity of the video.

## Proposed System for late fusion methods
![LateFusion_Proposed_System drawio](https://github.com/Cyriloo7/DeepFake-Detection-Using-Vision-and-Wav2Vec2-Transformer-Models/assets/91458980/06925cf5-3db8-46bd-997b-be35607fbc1b)

## Proposed System for early fusion methods
![EarlyFusion_Proposed_System drawio](https://github.com/Cyriloo7/DeepFake-Detection-Using-Vision-and-Wav2Vec2-Transformer-Models/assets/91458980/ed039a9b-d58f-4e5a-a427-c7b34c60add6)

